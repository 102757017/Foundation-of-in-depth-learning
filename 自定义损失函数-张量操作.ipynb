{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: UTF-8 -*-\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量与矩阵的相互转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([[1,2,3],[4,5,6]])\n",
    "#将矩阵转换为张量\n",
    "var1=K.variable(value=a,dtype=\"float32\")\n",
    "print(var1.shape)\n",
    "K.eval(var1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在下标为dim的轴上增加一维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "print(var1.shape)\n",
    "var2=K.expand_dims(var1,axis = 0)\n",
    "print(var2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建一个用[9,1,1]平铺的var2张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ExpandDims:0\", shape=(1, 2, 3), dtype=float32)\n",
      "Tensor(\"Tile_5:0\", shape=(9, 2, 3), dtype=float32)\n",
      "Tensor(\"Tile_6:0\", shape=(9, 2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(var2)\n",
    "var4=K.tile(var2,[9,1,1])\n",
    "print(var4)\n",
    "\n",
    "#快速复制张量\n",
    "var6=K.tile(K.expand_dims(var1,axis = 0),[9,1,1])\n",
    "print(var6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在下标为dim的轴上增加一维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 1, 3)\n",
      "Tensor(\"Tile_2:0\", shape=(2, 2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(var1.shape)\n",
    "var3=K.expand_dims(var1,axis = 1)\n",
    "print(var3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建一个用[1,9,1]平铺的var3张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ExpandDims_2:0\", shape=(2, 1, 3), dtype=float32)\n",
      "Tensor(\"Tile_7:0\", shape=(2, 9, 3), dtype=float32)\n",
      "Tensor(\"Tile_8:0\", shape=(2, 9, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(var3)\n",
    "var5=K.tile(var3,[1,9,1])\n",
    "print(var5)\n",
    "\n",
    "\n",
    "#快速复制张量\n",
    "var6=K.tile(K.expand_dims(var1,axis = 1),[1,9,1])\n",
    "print(var6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 对各个元素进行底数为e的指数运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  2.7182817,   7.389056 ,  20.085537 ],\n",
       "       [ 54.59815  , 148.41316  , 403.4288   ]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(K.eval(var1))\n",
    "K.eval(K.exp(var1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 求张量的最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(K.eval(var1))\n",
    "K.eval(K.max(var1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逐个元素对比两个张量，取较大值。如果是常量，则与与该常量逐个对比取较大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.7182817   7.389056   20.085537 ]\n",
      " [ 54.59815   148.41316   403.4288   ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 54.59815,  54.59815,  54.59815],\n",
       "       [ 54.59815, 148.41316, 403.4288 ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(K.eval(K.exp(var1)))\n",
    "K.eval(K.exp(K.maximum(var1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K.cast(x, dtype)转换张量的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var=K.variable(value=np.array([False,True]),dtype=\"bool\")\n",
    "print(K.eval(var))\n",
    "var=K.cast(var, dtype=\"int32\")\n",
    "K.eval(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K.sum求张量内所有元素的和\n",
    "axis = -2 对应 axis = 0,效果一致；  \n",
    "axis = -1 对应 axis = 1,效果一致；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 6]\n",
      " [1 6]]\n",
      "22\n",
      "[[10 12]]\n",
      "[[15]\n",
      " [ 7]]\n",
      "[[15]\n",
      " [ 7]]\n"
     ]
    }
   ],
   "source": [
    "var=np.random.randint(10,size=(2,2))\n",
    "var=K.variable(value=var,dtype=\"uint8\")\n",
    "print(K.eval(var))\n",
    "#所有元素求和\n",
    "print(K.eval(K.sum(var)))\n",
    "#按axis求和\n",
    "print(K.eval(K.sum(var,axis=0, keepdims=True)))\n",
    "print(K.eval(K.sum(var,axis=1, keepdims=True)))\n",
    "print(K.eval(K.sum(var,axis=-1, keepdims=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 生成服从正态分布的随机张量,mean为均值，std为标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.31489232 -0.20817436]\n",
      " [ 0.00971632  0.9129183 ]]\n",
      "[[-1.081441   -0.9185018 ]\n",
      " [ 0.87874234 -1.2285942 ]]\n"
     ]
    }
   ],
   "source": [
    "var=K.random_normal(shape=(2,2),mean=0.,stddev=1.0)\n",
    "print(K.eval(var))\n",
    "#注意每次print时均会重新取值\n",
    "print(K.eval(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成均匀分布的张量,minval为最小值，maxval为最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.32498205 0.8851963 ]\n",
      " [0.5817976  0.37890995]]\n",
      "[[0.7202232  0.3799771 ]\n",
      " [0.81245685 0.02957261]]\n"
     ]
    }
   ],
   "source": [
    "var=K.random_uniform(shape=(2,2),minval=0.,maxval=1.0)\n",
    "print(K.eval(var))\n",
    "#注意每次print时均会重新取值\n",
    "print(K.eval(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ones_like生成与目标shape一致的全1张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "kvar=K.random_uniform(shape=(2,2),minval=0.,maxval=1.0)\n",
    "kvar=K.ones_like(kvar)\n",
    "print(K.eval(kvar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K.greater 比较张量的大小生成bool值的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True False]]\n"
     ]
    }
   ],
   "source": [
    "var1=K.random_uniform(shape=(1,5),minval=0.,maxval=10.)\n",
    "print(K.eval(K.greater(var1,3.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 返回一个大于0的极小浮点数，防止除0错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 欧式距离$d_{12}=\\sqrt{\\sum_{k=1}^{n}\\left(x_{1 k}-x_{2 k}\\right)^{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欧式距离 ： 7.9435506\n"
     ]
    }
   ],
   "source": [
    "var1=np.asarray([6.5,4.2,7.4,3.5],np.float32)\n",
    "var2=np.array([1,3,2,5],np.float32)\n",
    "kvar1=K.variable(value=var1,dtype=\"float32\")\n",
    "kvar2=K.variable(value=var2,dtype=\"float32\")\n",
    "\n",
    "#使用公式求解\n",
    "L=K.sqrt(K.sum(K.square(kvar1-kvar2)))\n",
    "print(\"欧式距离 ：\",K.eval(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算张量的softmax值(每行转换为总和为1的概率分布）\n",
    "$\n",
    "假设我们有一个数组，V，Vi表示V中的第i个元素，那么这个元素的Softmax值就是  S_{i}=\\frac{e^{V_{i}}}{\\sum_{j} e^{V_{j}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.701295  5.4655223]\n",
      " [7.227085  6.6768413]]\n",
      "[[0.34215495 0.657845  ]\n",
      " [0.97683674 0.02316327]]\n",
      "[0.47536692 0.17487772 0.17487772 0.17487772]\n"
     ]
    }
   ],
   "source": [
    "var=K.random_uniform(shape=(2,2),minval=5,maxval=10)\n",
    "print(K.eval(var))\n",
    "var=K.softmax(var)\n",
    "print(K.eval(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE(均方差)   \n",
    "$M S E=S S E / n=\\frac{1}{n} \\sum_{i=1}^{n} w_{i}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 28.725\n"
     ]
    }
   ],
   "source": [
    "var1=np.asarray([6.5,4.2,7.4,3.5],np.float32)\n",
    "var2=np.array([1,0,0,0],np.float32)\n",
    "kvar1=K.variable(value=var1,dtype=\"float32\")\n",
    "kvar2=K.variable(value=var2,dtype=\"float32\")\n",
    "\n",
    "#使用keras预置的损失函数\n",
    "from keras import losses\n",
    "loss=K.eval(losses.mean_squared_error(kvar2,kvar1))\n",
    "print(\"loss:\",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSLE（均方对数损失）\n",
    "msle相比与mse的改进:如果想要预测的值范围很大,mse会受到一些大的值的引导,即使小的值预测准也不行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.8141832\n"
     ]
    }
   ],
   "source": [
    "var1=np.asarray([6.5,4.2,7.4,3.5],np.float32)\n",
    "var2=np.array([1,0,0,0],np.float32)\n",
    "kvar1=K.variable(value=var1,dtype=\"float32\")\n",
    "kvar2=K.variable(value=var2,dtype=\"float32\")\n",
    "\n",
    "#使用keras预置的损失函数\n",
    "from keras import losses\n",
    "loss=K.eval(losses.mean_squared_logarithmic_error(kvar2,kvar1))\n",
    "print(\"loss:\",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE(均方根)\n",
    "$R M S E=\\sqrt{M S E}=\\sqrt{S S E / n}=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} w_{i}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.359571\n"
     ]
    }
   ],
   "source": [
    "var1=np.asarray([6.5,4.2,7.4,3.5],np.float32)\n",
    "var2=np.array([1,0,0,0],np.float32)\n",
    "kvar1=K.variable(value=var1,dtype=\"float32\")\n",
    "kvar2=K.variable(value=var2,dtype=\"float32\")\n",
    "\n",
    "#使用keras预置的损失函数\n",
    "from keras import losses\n",
    "loss=K.sqrt(losses.mean_squared_error(kvar2,kvar1))\n",
    "print(\"loss:\",K.eval(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAE(平均绝对误差)\n",
    "$\\mathrm{MAE}=\\frac{\\sum_{i=1}^{n}\\left|y_{i}-x_{i}\\right|}{n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.15\n"
     ]
    }
   ],
   "source": [
    "var1=np.asarray([6.5,4.2,7.4,3.5],np.float32)\n",
    "var2=np.array([1,0,0,0],np.float32)\n",
    "kvar1=K.variable(value=var1,dtype=\"float32\")\n",
    "kvar2=K.variable(value=var2,dtype=\"float32\")\n",
    "\n",
    "#使用keras预置的损失函数\n",
    "from keras import losses\n",
    "loss=K.eval(losses.mean_absolute_error(kvar2,kvar1))\n",
    "print(\"loss:\",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# binary_crossentropy计算两个张量的二分类交叉熵,默认输入的张量应该是进行了概率编码的张量（总和为1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "二分类交叉熵: 0.6173691\n"
     ]
    }
   ],
   "source": [
    "var1=np.asarray([6.5,4.2,7.4,3.5],np.float32)\n",
    "var2=np.array([1,0,0,0],np.float32)\n",
    "kvar1=K.variable(value=var1,dtype=\"float32\")\n",
    "kvar2=K.variable(value=var2,dtype=\"float32\")\n",
    "\n",
    "#使用keras预置的损失函数\n",
    "from keras import losses\n",
    "#参数1为真实分布（需要进行独热编码）\n",
    "#参数2为预测值（需要进行softmax编码）\n",
    "loss=K.eval(losses.binary_crossentropy(kvar2,K.softmax(kvar1)))\n",
    "print(\"二分类交叉熵:\",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# categorical_crossentropy计算两个张量的多分类交叉熵，需要注意p，q的顺序，交换顺序会导致结果变化\n",
    "$\n",
    "H(p, q)=-\\sum_{x} p(x) \\log q(x)\n",
    "$  \n",
    "p为真实分布，q为非真实分布(预测值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 1.2836106\n",
      "kvar1\n",
      " [6.5 4.2 7.4 3.5]\n",
      "kvar2\n",
      " [1. 0. 0. 0.]\n",
      "loss2: 1.2836106\n",
      "loss3: 1.2836106\n"
     ]
    }
   ],
   "source": [
    "var1=np.asarray([6.5,4.2,7.4,3.5],np.float32)\n",
    "var2=np.array([1,0,0,0],np.float32)\n",
    "\n",
    "#方法一：使用numpy求解\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "loss1=-np.sum(var2*np.log(softmax(var1)))\n",
    "print(\"loss1:\",loss1)\n",
    "\n",
    "\n",
    "\n",
    "kvar1=K.variable(value=var1,dtype=\"float32\")\n",
    "kvar2=K.variable(value=var2,dtype=\"float32\")\n",
    "print(\"kvar1\\n\",K.eval(kvar1))\n",
    "print(\"kvar2\\n\",K.eval(kvar2))\n",
    "\n",
    "#方法二：使用tensorflow求解\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#参数1为真实分布（需要进行独热编码）\n",
    "#参数2为预测值（不需要进行softmax编码）\n",
    "loss2=sess.run(tf.losses.softmax_cross_entropy(kvar2,kvar1))\n",
    "sess.close()\n",
    "print(\"loss2:\",loss2)\n",
    "\n",
    "#方法三：使用keras预置的损失函数\n",
    "from keras import losses\n",
    "#参数1为真实分布（需要进行独热编码）\n",
    "#参数2为预测值（需要进行softmax编码）\n",
    "loss3=K.eval(losses.categorical_crossentropy(kvar2,K.softmax(kvar1)))\n",
    "print(\"loss3:\",loss3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL散度又称为相对熵，是交叉熵与信息熵的差值，相对熵=交叉熵-信息熵\n",
    "$\n",
    "D(p \\| q)=\\sum_{i=1}^{n} p(x) \\log \\frac{p(x)}{q(x)}\n",
    "$   \n",
    "p为真实分布，q为非真实分布(预测值)，KL散度是不对称的，交换p、q值会变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1： 3.4028\n",
      "kvar1\n",
      " [0.27703524 0.02777523 0.6813968  0.01379277]\n",
      "kvar2\n",
      " [0.01521943 0.11245721 0.04137069 0.83095264]\n",
      "loss2: 3.4027998\n",
      "loss3: 3.4027998\n"
     ]
    }
   ],
   "source": [
    "var1=np.asarray([6.5,4.2,7.4,3.5],np.float32)\n",
    "var2=np.array([1,3,2,5],np.float32)\n",
    "\n",
    "#方法一：使用numpy求解\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "loss1=np.sum(softmax(var2)*np.log(softmax(var2)/softmax(var1)))\n",
    "print(\"loss1：\",loss1)\n",
    "\n",
    "kvar1=K.variable(value=var1,dtype=\"float32\")\n",
    "kvar2=K.variable(value=var2,dtype=\"float32\")\n",
    "kvar1=K.softmax(kvar1)\n",
    "kvar2=K.softmax(kvar2)\n",
    "print(\"kvar1\\n\",K.eval(kvar1))\n",
    "print(\"kvar2\\n\",K.eval(kvar2))\n",
    "\n",
    "#方法二：使用keras手工计算，kullback_leibler_divergence\n",
    "loss2=K.sum(kvar2*K.log(kvar2/kvar1))\n",
    "print(\"loss2:\",K.eval( loss2))\n",
    "\n",
    "\n",
    "#方法三：使用keras预置的损失函数\n",
    "from keras import losses\n",
    "#参数1为真实分布（需要进行softmax编码）\n",
    "#参数2为预测值（需要进行softmax编码）\n",
    "loss3=losses.kullback_leibler_divergence(kvar2,kvar1)\n",
    "print(\"loss3:\",K.eval( loss3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# JS散度在KL散度的基础上进行了改进，具有对称性\n",
    "$J S\\left(P_{1} \\| P_{2}\\right)=\\frac{1}{2} K L\\left(P_{1} \\| \\frac{P_{1}+P_{2}}{2}\\right)+\\frac{1}{2} K L\\left(P_{2} \\| \\frac{P_{1}+P_{2}}{2}\\right)$  \n",
    "p为真实分布，q为非真实分布(预测值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1： 3.010124683380127\n",
      "kvar1\n",
      " [0.27703524 0.02777523 0.6813968  0.01379277]\n",
      "kvar2\n",
      " [0.01521943 0.11245721 0.04137069 0.83095264]\n",
      "loss2: 3.0101247\n",
      "loss3: 3.0101247\n"
     ]
    }
   ],
   "source": [
    "var1=np.asarray([6.5,4.2,7.4,3.5],np.float32)\n",
    "var2=np.array([1,3,2,5],np.float32)\n",
    "\n",
    "#方法一：使用numpy求解\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "loss1=0.5*np.sum(softmax(var2)*np.log(softmax(var2)/softmax(var1)))+0.5*np.sum(softmax(var1)*np.log(softmax(var1)/softmax(var2)))\n",
    "print(\"loss1：\",loss1)\n",
    "\n",
    "kvar1=K.variable(value=var1,dtype=\"float32\")\n",
    "kvar2=K.variable(value=var2,dtype=\"float32\")\n",
    "kvar1=K.softmax(kvar1)\n",
    "kvar2=K.softmax(kvar2)\n",
    "print(\"kvar1\\n\",K.eval(kvar1))\n",
    "print(\"kvar2\\n\",K.eval(kvar2))\n",
    "\n",
    "#方法二：使用keras手工计算，kullback_leibler_divergence\n",
    "loss2=0.5*K.sum(kvar2*K.log(kvar2/kvar1))+0.5*K.sum(kvar1*K.log(kvar1/kvar2))\n",
    "print(\"loss2:\",K.eval( loss2))\n",
    "\n",
    "#方法三：使用keras预置的损失函数\n",
    "from keras import losses\n",
    "loss3=0.5*losses.kullback_leibler_divergence(kvar2,kvar1)+0.5*losses.kullback_leibler_divergence(kvar1,kvar2)\n",
    "print(\"loss3:\",K.eval( loss3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstein 距离简称EMD，用来表示两个分布的相似程度。衡量了把数据从分布“移动成”分布时所需要移动的平均距离的最小值（类似于把一堆土从一个形状移动到另一个形状所需要做的功的最小值），Wasserstein 距离相比js散度的优点在于即使两个分布没有重叠，或重叠的部分很小，仍能反映两个分布的远近\n",
    "我们可以构造一个含参数w、最后一层不是非线性激活层的判别器网络f_w，在限制w不超过某个范围的条件下，使得\n",
    "\n",
    "$L = \\mathbb{E}_{x \\sim P_r} [f_w(x)] - \\mathbb{E}_{x \\sim P_g} [f_w(x)]$尽可能取到最大，此时L就会近似真实分布与生成分布之间的Wasserstein距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
